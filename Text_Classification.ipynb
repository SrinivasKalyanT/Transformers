{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLYplEP47-pt",
        "outputId": "ce625425-d090-49cd-f49a-a60efbd782de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "!pip install accelerate -U\n",
        "!pip install datasets\n",
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "id": "h408iJf276RW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efe2dfd4-5e17-4cf5-b84d-2b474bb08c5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.2)\n",
            "Collecting xxhash (from evaluate)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.0)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.3)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, responses, multiprocess, datasets, evaluate\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 evaluate-0.4.1 multiprocess-0.70.16 responses-0.18.0 xxhash-3.4.1\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.28.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 193830, done.\u001b[K\n",
            "remote: Counting objects: 100% (24477/24477), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2176/2176), done.\u001b[K\n",
            "remote: Total 193830 (delta 23855), reused 22319 (delta 22293), pack-reused 169353\u001b[K\n",
            "Receiving objects: 100% (193830/193830), 202.65 MiB | 25.11 MiB/s, done.\n",
            "Resolving deltas: 100% (138346/138346), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "\n",
        "zip_file_name = '/content/NLI_Dataset.zip'\n",
        "\n",
        "\n",
        "drive_folder = '/content/data'\n",
        "\n",
        "\n",
        "with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/data')\n",
        "\n",
        "\n",
        "extracted_folder = '/content/data'\n",
        "for file in os.listdir(extracted_folder):\n",
        "    full_path = os.path.join(extracted_folder, file)\n",
        "    destination_path = os.path.join(drive_folder, file)\n",
        "    os.rename(full_path, destination_path)"
      ],
      "metadata": {
        "id": "dOrP5A4q9csH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python '/content/transformers/examples/pytorch/text-classification/run_glue.py' \\\n",
        "  --model_name_or_path google-bert/bert-base-cased \\\n",
        "  --dataset_name imdb  \\\n",
        "  --do_train \\\n",
        "  --do_predict \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 1e-4 \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --output_dir '/content/drive/MyDrive/NLI_Folder'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekrw9h-EdACS",
        "outputId": "53d2400e-c742-4adc-af59-abe988a387b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-04 19:06:18.401486: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-04 19:06:18.401552: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-04 19:06:18.537488: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-04 19:06:20.230340: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/04/2024 19:06:25 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/04/2024 19:06:25 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0001,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/drive/MyDrive/NLI_Folder/runs/Apr04_19-06-24_7e42259cd700,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/drive/MyDrive/NLI_Folder,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/drive/MyDrive/NLI_Folder,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "https://huggingface.co/datasets/imdb/resolve/e6281661ce1c48d982bc483cf8a173c1bbeb5d31/README.md not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/fd42c192c77f3323078fbc453517e0367960a26a9a76f16f392b457f1f1590db.21313d55bf9978e09c26dbe72814a4215f94b46e893c09791b0d639854ccdc98.incomplete\n",
            "04/04/2024 19:06:27 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/imdb/resolve/e6281661ce1c48d982bc483cf8a173c1bbeb5d31/README.md not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/fd42c192c77f3323078fbc453517e0367960a26a9a76f16f392b457f1f1590db.21313d55bf9978e09c26dbe72814a4215f94b46e893c09791b0d639854ccdc98.incomplete\n",
            "Downloading readme: 100% 7.81k/7.81k [00:00<00:00, 27.0MB/s]\n",
            "storing https://huggingface.co/datasets/imdb/resolve/e6281661ce1c48d982bc483cf8a173c1bbeb5d31/README.md in cache at /root/.cache/huggingface/datasets/downloads/fd42c192c77f3323078fbc453517e0367960a26a9a76f16f392b457f1f1590db.21313d55bf9978e09c26dbe72814a4215f94b46e893c09791b0d639854ccdc98\n",
            "04/04/2024 19:06:27 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/imdb/resolve/e6281661ce1c48d982bc483cf8a173c1bbeb5d31/README.md in cache at /root/.cache/huggingface/datasets/downloads/fd42c192c77f3323078fbc453517e0367960a26a9a76f16f392b457f1f1590db.21313d55bf9978e09c26dbe72814a4215f94b46e893c09791b0d639854ccdc98\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/fd42c192c77f3323078fbc453517e0367960a26a9a76f16f392b457f1f1590db.21313d55bf9978e09c26dbe72814a4215f94b46e893c09791b0d639854ccdc98\n",
            "04/04/2024 19:06:27 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/fd42c192c77f3323078fbc453517e0367960a26a9a76f16f392b457f1f1590db.21313d55bf9978e09c26dbe72814a4215f94b46e893c09791b0d639854ccdc98\n",
            "Generating dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/0.0.0/e6281661ce1c48d982bc483cf8a173c1bbeb5d31)\n",
            "04/04/2024 19:06:31 - INFO - datasets.builder - Generating dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/0.0.0/e6281661ce1c48d982bc483cf8a173c1bbeb5d31)\n",
            "Downloading and preparing dataset imdb/plain_text to /root/.cache/huggingface/datasets/imdb/plain_text/0.0.0/e6281661ce1c48d982bc483cf8a173c1bbeb5d31...\n",
            "04/04/2024 19:06:31 - INFO - datasets.builder - Downloading and preparing dataset imdb/plain_text to /root/.cache/huggingface/datasets/imdb/plain_text/0.0.0/e6281661ce1c48d982bc483cf8a173c1bbeb5d31...\n",
            "Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "04/04/2024 19:06:32 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/train-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/859464a68e3b85a26cacaacbb19286502f5878c76c415f6adf986b6534ce43c6.incomplete\n",
            "04/04/2024 19:06:33 - INFO - datasets.utils.file_utils - hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/train-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/859464a68e3b85a26cacaacbb19286502f5878c76c415f6adf986b6534ce43c6.incomplete\n",
            "Downloading data: 100% 21.0M/21.0M [00:02<00:00, 7.54MB/s]\n",
            "storing hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/train-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/859464a68e3b85a26cacaacbb19286502f5878c76c415f6adf986b6534ce43c6\n",
            "04/04/2024 19:06:37 - INFO - datasets.utils.file_utils - storing hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/train-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/859464a68e3b85a26cacaacbb19286502f5878c76c415f6adf986b6534ce43c6\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/859464a68e3b85a26cacaacbb19286502f5878c76c415f6adf986b6534ce43c6\n",
            "04/04/2024 19:06:37 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/859464a68e3b85a26cacaacbb19286502f5878c76c415f6adf986b6534ce43c6\n",
            "hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/test-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/c7db353801bd03ee861032883704093b77b7e2c2afc907f4506a7fb276f170af.incomplete\n",
            "04/04/2024 19:06:37 - INFO - datasets.utils.file_utils - hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/test-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/c7db353801bd03ee861032883704093b77b7e2c2afc907f4506a7fb276f170af.incomplete\n",
            "Downloading data: 100% 20.5M/20.5M [00:01<00:00, 12.8MB/s]\n",
            "storing hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/test-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/c7db353801bd03ee861032883704093b77b7e2c2afc907f4506a7fb276f170af\n",
            "04/04/2024 19:06:40 - INFO - datasets.utils.file_utils - storing hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/test-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/c7db353801bd03ee861032883704093b77b7e2c2afc907f4506a7fb276f170af\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/c7db353801bd03ee861032883704093b77b7e2c2afc907f4506a7fb276f170af\n",
            "04/04/2024 19:06:40 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/c7db353801bd03ee861032883704093b77b7e2c2afc907f4506a7fb276f170af\n",
            "hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/unsupervised-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/15565b79678a88706c823c19a71f090f89d1a46a849c6d5aecef60b69eed093e.incomplete\n",
            "04/04/2024 19:06:41 - INFO - datasets.utils.file_utils - hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/unsupervised-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/15565b79678a88706c823c19a71f090f89d1a46a849c6d5aecef60b69eed093e.incomplete\n",
            "Downloading data: 100% 42.0M/42.0M [00:02<00:00, 15.4MB/s]\n",
            "storing hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/unsupervised-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/15565b79678a88706c823c19a71f090f89d1a46a849c6d5aecef60b69eed093e\n",
            "04/04/2024 19:06:45 - INFO - datasets.utils.file_utils - storing hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/unsupervised-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/15565b79678a88706c823c19a71f090f89d1a46a849c6d5aecef60b69eed093e\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/15565b79678a88706c823c19a71f090f89d1a46a849c6d5aecef60b69eed093e\n",
            "04/04/2024 19:06:45 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/15565b79678a88706c823c19a71f090f89d1a46a849c6d5aecef60b69eed093e\n",
            "Downloading took 0.0 min\n",
            "04/04/2024 19:06:45 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "04/04/2024 19:06:45 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Generating train split\n",
            "04/04/2024 19:06:45 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 100% 25000/25000 [00:00<00:00, 149211.24 examples/s]\n",
            "Generating test split\n",
            "04/04/2024 19:06:45 - INFO - datasets.builder - Generating test split\n",
            "Generating test split: 100% 25000/25000 [00:00<00:00, 169754.62 examples/s]\n",
            "Generating unsupervised split\n",
            "04/04/2024 19:06:45 - INFO - datasets.builder - Generating unsupervised split\n",
            "Generating unsupervised split: 100% 50000/50000 [00:00<00:00, 171748.62 examples/s]\n",
            "All the splits matched successfully.\n",
            "04/04/2024 19:06:45 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/0.0.0/e6281661ce1c48d982bc483cf8a173c1bbeb5d31. Subsequent calls will reuse this data.\n",
            "04/04/2024 19:06:45 - INFO - datasets.builder - Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/0.0.0/e6281661ce1c48d982bc483cf8a173c1bbeb5d31. Subsequent calls will reuse this data.\n",
            "config.json: 100% 570/570 [00:00<00:00, 2.27MB/s]\n",
            "[INFO|configuration_utils.py:728] 2024-04-04 19:06:46,084 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json\n",
            "[INFO|configuration_utils.py:791] 2024-04-04 19:06:46,094 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"google-bert/bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "tokenizer_config.json: 100% 49.0/49.0 [00:00<00:00, 173kB/s]\n",
            "[INFO|configuration_utils.py:728] 2024-04-04 19:06:46,537 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json\n",
            "[INFO|configuration_utils.py:791] 2024-04-04 19:06:46,539 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"google-bert/bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "vocab.txt: 100% 213k/213k [00:00<00:00, 1.02MB/s]\n",
            "tokenizer.json: 100% 436k/436k [00:00<00:00, 1.09MB/s]\n",
            "[INFO|tokenization_utils_base.py:2046] 2024-04-04 19:06:48,604 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2046] 2024-04-04 19:06:48,604 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2046] 2024-04-04 19:06:48,604 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2046] 2024-04-04 19:06:48,604 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2046] 2024-04-04 19:06:48,604 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:728] 2024-04-04 19:06:48,605 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json\n",
            "[INFO|configuration_utils.py:791] 2024-04-04 19:06:48,606 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"google-bert/bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "model.safetensors: 100% 436M/436M [00:04<00:00, 89.2MB/s]\n",
            "[INFO|modeling_utils.py:3257] 2024-04-04 19:06:53,945 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/model.safetensors\n",
            "[INFO|modeling_utils.py:3982] 2024-04-04 19:06:54,719 >> Some weights of the model checkpoint at google-bert/bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3994] 2024-04-04 19:06:54,719 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/25000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/0.0.0/e6281661ce1c48d982bc483cf8a173c1bbeb5d31/cache-02db8d87a2eeb54f.arrow\n",
            "04/04/2024 19:06:55 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/0.0.0/e6281661ce1c48d982bc483cf8a173c1bbeb5d31/cache-02db8d87a2eeb54f.arrow\n",
            "Running tokenizer on dataset: 100% 25000/25000 [00:22<00:00, 1119.64 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/25000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/0.0.0/e6281661ce1c48d982bc483cf8a173c1bbeb5d31/cache-cc427463e9a0de55.arrow\n",
            "04/04/2024 19:07:18 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/0.0.0/e6281661ce1c48d982bc483cf8a173c1bbeb5d31/cache-cc427463e9a0de55.arrow\n",
            "Running tokenizer on dataset: 100% 25000/25000 [00:19<00:00, 1253.64 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/50000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/0.0.0/e6281661ce1c48d982bc483cf8a173c1bbeb5d31/cache-7fcbf5511cdfeee6.arrow\n",
            "04/04/2024 19:07:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/0.0.0/e6281661ce1c48d982bc483cf8a173c1bbeb5d31/cache-7fcbf5511cdfeee6.arrow\n",
            "Running tokenizer on dataset: 100% 50000/50000 [00:41<00:00, 1216.98 examples/s]\n",
            "04/04/2024 19:08:18 - INFO - __main__ - Sample 20952 of the training set: {'text': 'Arguably this is a very good \"sequel\", better than the first live action film 101 Dalmatians. It has good dogs, good actors, good jokes and all right slapstick! <br /><br />Cruella DeVil, who has had some rather major therapy, is now a lover of dogs and very kind to them. Many, including Chloe Simon, owner of one of the dogs that Cruella once tried to kill, do not believe this. Others, like Kevin Shepherd (owner of 2nd Chance Dog Shelter) believe that she has changed. <br /><br />Meanwhile, Dipstick, with his mate, have given birth to three cute dalmatian puppies! Little Dipper, Domino and Oddball...<br /><br />Starring Eric Idle as Waddlesworth (the hilarious macaw), Glenn Close as Cruella herself and Gerard Depardieu as Le Pelt (another baddie, the name should give a clue), this is a good family film with excitement and lots more!! One downfall of this film is that is has a lot of painful slapstick, but not quite as excessive as the last film. This is also funnier than the last film.<br /><br />Enjoy \"102 Dalmatians\"! :-)', 'label': 1, 'input_ids': [101, 138, 10805, 6718, 4999, 1142, 1110, 170, 1304, 1363, 107, 8047, 107, 117, 1618, 1190, 1103, 1148, 1686, 2168, 1273, 7393, 25938, 21943, 5895, 119, 1135, 1144, 1363, 6363, 117, 1363, 5681, 117, 1363, 13948, 1105, 1155, 1268, 15933, 23743, 106, 133, 9304, 120, 135, 133, 9304, 120, 135, 140, 26930, 3848, 3177, 2559, 2723, 117, 1150, 1144, 1125, 1199, 1897, 1558, 7606, 117, 1110, 1208, 170, 7559, 1104, 6363, 1105, 1304, 1912, 1106, 1172, 119, 2408, 117, 1259, 9344, 3274, 117, 3172, 1104, 1141, 1104, 1103, 6363, 1115, 140, 26930, 3848, 1517, 1793, 1106, 2311, 117, 1202, 1136, 2059, 1142, 119, 8452, 117, 1176, 4101, 13444, 113, 3172, 1104, 2518, 12423, 8166, 1153, 18041, 114, 2059, 1115, 1131, 1144, 2014, 119, 133, 9304, 120, 135, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n",
            "04/04/2024 19:08:18 - INFO - __main__ - Sample 3648 of the training set: {'text': \"It's a good thing I didn't watch this while i was pregnant.I definitely would have cried my eyes out and/or vomit. It was Kind of gruesome mainly disturbing. I personally thought the baby was adorable in its own twisted little way.However as a mom I cringed when Beth stabbed herself in the stomach and when Virgina aborted the child during her 3rd trimester with rusty utensils no less.Also,as an animal lover i almost cried when she scratched the cat to a bloody pulp.However,As creepy and sinister as the baby was I was rooting for it to live.And as twisted as the movie was I am extremely intrigued to see the sequel...... ......... ....... ......... ......... ....... ...... .....\", 'label': 0, 'input_ids': [101, 1135, 112, 188, 170, 1363, 1645, 146, 1238, 112, 189, 2824, 1142, 1229, 178, 1108, 6391, 119, 146, 5397, 1156, 1138, 6104, 1139, 1257, 1149, 1105, 120, 1137, 26979, 119, 1135, 1108, 15994, 1104, 176, 26930, 11743, 2871, 15958, 119, 146, 7572, 1354, 1103, 2963, 1108, 27627, 1107, 1157, 1319, 6061, 1376, 1236, 119, 1438, 1112, 170, 4113, 146, 26881, 1165, 6452, 13699, 1941, 1107, 1103, 3472, 1105, 1165, 6567, 1161, 170, 12207, 1906, 1103, 2027, 1219, 1123, 2973, 13373, 12831, 1114, 26195, 190, 23826, 8825, 1185, 1750, 119, 2907, 117, 1112, 1126, 3724, 7559, 178, 1593, 6104, 1165, 1131, 15844, 1103, 5855, 1106, 170, 7201, 20467, 119, 1438, 117, 1249, 19857, 1105, 20975, 1112, 1103, 2963, 1108, 146, 1108, 7261, 1158, 1111, 1122, 1106, 1686, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n",
            "04/04/2024 19:08:18 - INFO - __main__ - Sample 819 of the training set: {'text': \"This has to be the worst movie I have seen. Madsen fans don't be drawn into this like I was. He is only in it for a maximum of five minutes. This movie is so bad that the only reason why you would watch it is if all the rest of the movies on earth as well as t.v. had been destroyed.\", 'label': 0, 'input_ids': [101, 1188, 1144, 1106, 1129, 1103, 4997, 2523, 146, 1138, 1562, 119, 10779, 3792, 3899, 1274, 112, 189, 1129, 3795, 1154, 1142, 1176, 146, 1108, 119, 1124, 1110, 1178, 1107, 1122, 1111, 170, 4177, 1104, 1421, 1904, 119, 1188, 2523, 1110, 1177, 2213, 1115, 1103, 1178, 2255, 1725, 1128, 1156, 2824, 1122, 1110, 1191, 1155, 1103, 1832, 1104, 1103, 5558, 1113, 4033, 1112, 1218, 1112, 189, 119, 191, 119, 1125, 1151, 3072, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "Downloading builder script: 100% 4.20k/4.20k [00:00<00:00, 15.7MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:759] 2024-04-04 19:08:21,162 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1812] 2024-04-04 19:08:21,171 >> ***** Running training *****\n",
            "[INFO|trainer.py:1813] 2024-04-04 19:08:21,171 >>   Num examples = 25,000\n",
            "[INFO|trainer.py:1814] 2024-04-04 19:08:21,172 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:1815] 2024-04-04 19:08:21,172 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1818] 2024-04-04 19:08:21,172 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1819] 2024-04-04 19:08:21,172 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1820] 2024-04-04 19:08:21,172 >>   Total optimization steps = 782\n",
            "[INFO|trainer.py:1821] 2024-04-04 19:08:21,172 >>   Number of trainable parameters = 108,311,810\n",
            "{'loss': 0.397, 'grad_norm': 5.408255577087402, 'learning_rate': 3.60613810741688e-05, 'epoch': 0.64}\n",
            " 64% 500/782 [05:18<03:10,  1.48it/s][INFO|trainer.py:3067] 2024-04-04 19:13:40,089 >> Saving model checkpoint to /content/drive/MyDrive/NLI_Folder/tmp-checkpoint-500\n",
            "[INFO|configuration_utils.py:473] 2024-04-04 19:13:40,096 >> Configuration saved in /content/drive/MyDrive/NLI_Folder/tmp-checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:2454] 2024-04-04 19:13:42,638 >> Model weights saved in /content/drive/MyDrive/NLI_Folder/tmp-checkpoint-500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2459] 2024-04-04 19:13:42,646 >> tokenizer config file saved in /content/drive/MyDrive/NLI_Folder/tmp-checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2468] 2024-04-04 19:13:42,652 >> Special tokens file saved in /content/drive/MyDrive/NLI_Folder/tmp-checkpoint-500/special_tokens_map.json\n",
            "100% 782/782 [08:34<00:00,  1.95it/s][INFO|trainer.py:2067] 2024-04-04 19:16:55,826 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 514.6799, 'train_samples_per_second': 48.574, 'train_steps_per_second': 1.519, 'train_loss': 0.36529825898387547, 'epoch': 1.0}\n",
            "100% 782/782 [08:34<00:00,  1.52it/s]\n",
            "[INFO|trainer.py:3067] 2024-04-04 19:16:55,859 >> Saving model checkpoint to /content/drive/MyDrive/NLI_Folder\n",
            "[INFO|configuration_utils.py:473] 2024-04-04 19:16:55,869 >> Configuration saved in /content/drive/MyDrive/NLI_Folder/config.json\n",
            "[INFO|modeling_utils.py:2454] 2024-04-04 19:16:58,093 >> Model weights saved in /content/drive/MyDrive/NLI_Folder/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2459] 2024-04-04 19:16:58,100 >> tokenizer config file saved in /content/drive/MyDrive/NLI_Folder/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2468] 2024-04-04 19:16:58,106 >> Special tokens file saved in /content/drive/MyDrive/NLI_Folder/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  train_loss               =     0.3653\n",
            "  train_runtime            = 0:08:34.67\n",
            "  train_samples            =      25000\n",
            "  train_samples_per_second =     48.574\n",
            "  train_steps_per_second   =      1.519\n",
            "04/04/2024 19:16:58 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:759] 2024-04-04 19:16:58,166 >> The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3376] 2024-04-04 19:16:58,169 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3378] 2024-04-04 19:16:58,170 >>   Num examples = 25000\n",
            "[INFO|trainer.py:3381] 2024-04-04 19:16:58,170 >>   Batch size = 8\n",
            "100% 3125/3125 [02:58<00:00, 17.51it/s]\n",
            "04/04/2024 19:19:56 - INFO - __main__ - ***** Predict results None *****\n",
            "[INFO|modelcard.py:450] 2024-04-04 19:19:57,252 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# export TASK_NAME=mrpc\n",
        "\n",
        "!python '/content/transformers/examples/pytorch/text-classification/run_glue_no_trainer.py' \\\n",
        "  --model_name_or_path '/content/drive/MyDrive/NLI_Folder/checkpoint-500' \\\n",
        "  --task_name 'sst2' \\\n",
        "  --train_file '/content/data/Train.csv' \\\n",
        "  --validation_file '/content/data/Valid.csv' \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 1e-4 \\\n",
        "  --num_train_epochs 2 \\\n",
        "  --output_dir '/content/drive/MyDrive/NLI_Folder'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StLeqiISjicI",
        "outputId": "3cdb2c80-983e-43c8-9396-23833b06bc4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-04 19:26:14.058212: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-04 19:26:14.058282: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-04 19:26:14.060273: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-04 19:26:15.924410: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/04/2024 19:26:21 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: no\n",
            "\n",
            "Downloading readme: 100% 35.3k/35.3k [00:00<00:00, 178kB/s]\n",
            "Downloading data: 100% 3.11M/3.11M [00:00<00:00, 7.79MB/s]\n",
            "Downloading data: 100% 72.8k/72.8k [00:00<00:00, 317kB/s]\n",
            "Downloading data: 100% 148k/148k [00:00<00:00, 642kB/s]\n",
            "Generating train split: 100% 67349/67349 [00:00<00:00, 1244382.00 examples/s]\n",
            "Generating validation split: 100% 872/872 [00:00<00:00, 436135.59 examples/s]\n",
            "Generating test split: 100% 1821/1821 [00:00<00:00, 671280.33 examples/s]\n",
            "loading configuration file /content/drive/MyDrive/NLI_Folder/checkpoint-500/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"/content/drive/MyDrive/NLI_Folder/checkpoint-500\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"finetuning_task\": \"sst2\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading file vocab.txt\n",
            "loading file tokenizer.json\n",
            "loading file added_tokens.json\n",
            "loading file special_tokens_map.json\n",
            "loading file tokenizer_config.json\n",
            "loading weights file /content/drive/MyDrive/NLI_Folder/checkpoint-500/model.safetensors\n",
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/NLI_Folder/checkpoint-500.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "Running tokenizer on dataset: 100% 67349/67349 [00:05<00:00, 13347.81 examples/s]\n",
            "Running tokenizer on dataset: 100% 872/872 [00:00<00:00, 12330.16 examples/s]\n",
            "Running tokenizer on dataset: 100% 1821/1821 [00:00<00:00, 4422.93 examples/s]\n",
            "04/04/2024 19:26:39 - INFO - __main__ - Sample 61943 of the training set: {'input_ids': [101, 14255, 23718, 1116, 1157, 3802, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1], 'labels': 0}.\n",
            "04/04/2024 19:26:39 - INFO - __main__ - Sample 8796 of the training set: {'input_ids': [101, 1294, 169, 22572, 9866, 2737, 112, 170, 1304, 1363, 113, 1133, 1136, 1632, 114, 2523, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.\n",
            "04/04/2024 19:26:39 - INFO - __main__ - Sample 63890 of the training set: {'input_ids': [101, 1115, 12169, 1113, 1177, 1242, 3001, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:571: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.\n",
            "  warnings.warn(\n",
            "Downloading builder script: 100% 5.75k/5.75k [00:00<00:00, 18.0MB/s]\n",
            "04/04/2024 19:26:42 - INFO - __main__ - ***** Running training *****\n",
            "04/04/2024 19:26:42 - INFO - __main__ -   Num examples = 67349\n",
            "04/04/2024 19:26:42 - INFO - __main__ -   Num Epochs = 2\n",
            "04/04/2024 19:26:42 - INFO - __main__ -   Instantaneous batch size per device = 32\n",
            "04/04/2024 19:26:42 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "04/04/2024 19:26:42 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
            "04/04/2024 19:26:42 - INFO - __main__ -   Total optimization steps = 4210\n",
            " 50% 2105/4210 [08:02<07:48,  4.49it/s]04/04/2024 19:34:47 - INFO - __main__ - epoch 0: {'accuracy': 0.8979357798165137}\n",
            "100% 4210/4210 [16:16<00:00,  4.03it/s]04/04/2024 19:43:01 - INFO - __main__ - epoch 1: {'accuracy': 0.9151376146788991}\n",
            "Configuration saved in /content/drive/MyDrive/NLI_Folder/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLI_Folder/model.safetensors\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLI_Folder/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLI_Folder/special_tokens_map.json\n",
            "100% 4210/4210 [16:21<00:00,  4.29it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "# Load the saved model\n",
        "model_path = '/content/drive/MyDrive/NLI_Folder/checkpoint-500'  # Path where the model is saved\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Load the test dataset\n",
        "test_dataset_path = '/content/data/Test.csv'  # Path to the test dataset\n",
        "# Load the dataset into a pandas DataFrame or any suitable data structure\n",
        "test_df = pd.read_csv(test_dataset_path)\n",
        "\n",
        "# Select a single document from the test dataset\n",
        "# document_index = 2 # Index of the document you want to evaluate\n",
        "document = test_df.iloc[8]['text']\n",
        "\n",
        "# Tokenize the single document\n",
        "tokenized_document = tokenizer(document, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "# Prepare input tensors\n",
        "input_ids = tokenized_document['input_ids']\n",
        "attention_masks = tokenized_document['attention_mask']\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids, attention_mask=attention_masks)\n",
        "\n",
        "# Get predicted label\n",
        "predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
        "\n",
        "# Map predicted label to class\n",
        "# Here, you may have to define a mapping between predicted labels and their corresponding classes.\n",
        "# For example, if the model outputs 0 for negative sentiment and 1 for positive sentiment:\n",
        "label_mapping = {0: 'Negative', 1: 'Positive'}\n",
        "predicted_class = label_mapping[predicted_label]\n",
        "\n",
        "print(\"Predicted sentiment:\", predicted_class)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp4WOL4Fk0_G",
        "outputId": "195f07d6-b820-4d32-e6c2-a49487cb035a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted sentiment: Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "# Load the saved model\n",
        "model_path = '/content/drive/MyDrive/NLI_Folder/checkpoint-500'  # Path where the model is saved\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Load the test dataset\n",
        "test_dataset_path = '/content/Test.csv'  # Path to the test dataset\n",
        "test_df = pd.read_csv(test_dataset_path)\n",
        "\n",
        "# Prepare lists to store predicted labels\n",
        "predicted_labels = []\n",
        "\n",
        "# Iterate over test dataset\n",
        "for index, row in test_df.iterrows():\n",
        "    # Tokenize the document\n",
        "    tokenized_text = tokenizer(row['text'], padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "    # Prepare input tensors\n",
        "    input_ids = tokenized_text['input_ids']\n",
        "    attention_masks = tokenized_text['attention_mask']\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_masks)\n",
        "\n",
        "    # Get predicted label\n",
        "    predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
        "\n",
        "    # Append predicted label to list\n",
        "    predicted_labels.append(predicted_label)\n",
        "\n",
        "# Add predicted labels to the test dataset\n",
        "test_df['predicted_values'] = predicted_labels\n",
        "\n",
        "# Save the updated test dataset with predicted labels\n",
        "test_df.to_csv('/content/Test.csv', index=False)\n",
        "\n",
        "print(\"Predicted values saved to 'Test_with_predictions.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2wD5DbFo1Hb",
        "outputId": "ad634765-a9b7-4e7e-a4a2-72a4a6637047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted values saved to 'Test_with_predictions.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "reoLYv4HqtsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/Test.csv')"
      ],
      "metadata": {
        "id": "aWuddmnS1-Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "MmuewVlI1_7H",
        "outputId": "293374c0-0fdd-4c7c-cca8-0df13eb7c0a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   text  label  \\\n",
              "0     I always wrote this series off as being a comp...      0   \n",
              "1     1st watched 12/7/2002 - 3 out of 10(Dir-Steve ...      0   \n",
              "2     This movie was so poorly written and directed ...      0   \n",
              "3     The most interesting thing about Miryang (Secr...      1   \n",
              "4     when i first read about \"berlin am meer\" i did...      0   \n",
              "...                                                 ...    ...   \n",
              "4995  This is the kind of picture John Lassiter woul...      1   \n",
              "4996  A MUST SEE! I saw WHIPPED at a press screening...      1   \n",
              "4997  NBC should be ashamed. I wouldn't allow my chi...      0   \n",
              "4998  This movie is a clumsy mishmash of various gho...      0   \n",
              "4999  Formula movie about the illegitimate son of a ...      0   \n",
              "\n",
              "      predicted_values  \n",
              "0                    0  \n",
              "1                    0  \n",
              "2                    0  \n",
              "3                    1  \n",
              "4                    0  \n",
              "...                ...  \n",
              "4995                 1  \n",
              "4996                 1  \n",
              "4997                 0  \n",
              "4998                 0  \n",
              "4999                 0  \n",
              "\n",
              "[5000 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e3820c39-411b-4059-a83c-61d87272fa44\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>predicted_values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I always wrote this series off as being a comp...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1st watched 12/7/2002 - 3 out of 10(Dir-Steve ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This movie was so poorly written and directed ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The most interesting thing about Miryang (Secr...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>when i first read about \"berlin am meer\" i did...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>This is the kind of picture John Lassiter woul...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4996</th>\n",
              "      <td>A MUST SEE! I saw WHIPPED at a press screening...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4997</th>\n",
              "      <td>NBC should be ashamed. I wouldn't allow my chi...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4998</th>\n",
              "      <td>This movie is a clumsy mishmash of various gho...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4999</th>\n",
              "      <td>Formula movie about the illegitimate son of a ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5000 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3820c39-411b-4059-a83c-61d87272fa44')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e3820c39-411b-4059-a83c-61d87272fa44 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e3820c39-411b-4059-a83c-61d87272fa44');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-218de2ea-33da-4000-baa0-8f34b1a10c84\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-218de2ea-33da-4000-baa0-8f34b1a10c84')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-218de2ea-33da-4000-baa0-8f34b1a10c84 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5000,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4995,\n        \"samples\": [\n          \"I'm the sort of person who went down to the local library and read books on Babbage's difference engine whilst my schoolmates were playing football etc.. So, if there is any such thing as a target audience for this film, then I guess I'd probably be included in that.<br /><br />Maybe I just need to watch it again. A previous reviewer mentioned not to watch this film whilst being tired. Maybe that was my mistake.<br /><br />I tried my best to enjoy this film, and there are aspects of it that I do like, but overall I found it amateurish and quite plodding.<br /><br />Being somewhat of a self confessed computer nerd, I just can't help but pick up on the exact time frame when the movie was actually made, and how the employed graphics reflect that time (i.e. 1997). Having played games of the era c.f. \\\"Mind Grind\\\" to cite one example, this film cannot escape that 16-bit colour low res multimedia explosion of that time. Now thankfully this has somewhat lessened in more recent years in the gaming world at least, in favour of actual game play.<br /><br />Having to resort to watching this movie via a German FTA satellite channel (as I don't think it's ever been aired on UK FTA TV, well not recently anyway), I was mildly amused to see the end credits note Gottdog (God dog) had 4 people working on it's design. Maybe it's mean spirited of me to be amused by this, given that ten years have elapsed since the movie was made, nevertheless the end result makes movie graphics from the eighties look good by comparison.<br /><br />But, as for the main story, I agree that the format isn't the best idea. Like others I agree that Ada deserves a film without the sci-fi angle, and a more straightforward biographical approach would perhaps be better suited to covering the life story of this remarkable lady.<br /><br />There are fundamental mistakes that undermine my enjoyment of this movie. First of all the underlying idea that somehow lost real-world information from the past can be accurately reconstructed through some sort of extrapolation via software based intelligent agents, seems somehow ludicrous.<br /><br />Also, the theme running through the movie that a computing device can indeed predict the mechanics of all things through the course of time (e.g. the winds) is now known not to be the case.<br /><br />OK, so the Victorians may have held this view, but the 20th century works of G\\u00f6del proving that no mathematical system can be complete, Turing's works on the limits of computability, not to mention chaos theory and quantum mechanics, have all completely undermined these ideas, which seem central to how the modern day researcher's software is supposed to work.<br /><br />Finally, the clicking of the mouse in the air to mean \\\"programming\\\" is also just plain wrong, as previously mentioned.<br /><br />This film maybe could have been OK, but at least some technical and scientific consultation would have given the film some much needed credit in the believability stakes.<br /><br />I won't forget the film though, as like \\\"Pi\\\", it is clearly a unique work, but with too many fatal mistakes for me to truly enjoy it, 3/10 from me.\",\n          \"I have two good things to say about this film: the scenery is beautiful and Peter Falk gives a good performance (considering what he had to work with in terms of dialog and direction). However, that said, I found this film extremely tiresome. Watching paint dry would have been more entertaining. It seemed much longer than 97 minutes. Beginning with opening sequence, where everyone is talking over each other and Paul Reiser is repeating everything that's said to him on the phone, the movie is annoying. The film is filled with clich\\u00e9s and shtick, not to mention endless incidents of audible flatulence by Falk. Also, the director seems to have had difficulty deciding whether to aim for laughs or tears. There are some sequences that are touching, but they're all played for laughs. If schmaltzy, sentimental, and \\\"cute\\\" appeal to you, you'll love it. But if you were hoping for something with more substance, see a different movie.\",\n          \"Strained and humorless (especially in light of its rather dubious psychology), but well-paced and comfortably lurid, this genteel body count movie highlights the unusually hypnotic presence of Angharad Rees as a young woman periodically possessed by Jack the Ripper, thus allowing for some nasty gore effects amidst the Edwardian propriety. It's all pretty standard stuff for Hammer, but is handled with a good deal of visual elan, even if the central relationship, between psychoanalyst Porter and Rees, drives the narrative without ever being satisfactorily explained.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted_values\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_labels = test_df['label']  # Replace 'label_column' with the actual column name containing true labels\n",
        "predicted_labels = test_df['predicted_values']\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGxB2fr72AN5",
        "outputId": "dadd2ee7-0e76-49f9-d1c1-82795a3c6ee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "OLhwNtyi2zRT",
        "outputId": "c2c1d325-a483-42bb-e19f-80a1c906c7ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TuBGCsT43AlN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}